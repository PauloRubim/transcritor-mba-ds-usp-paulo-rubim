{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11592614,"sourceType":"datasetVersion","datasetId":6739843},{"sourceId":279637,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":239546,"modelId":261197},{"sourceId":299595,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":256007,"modelId":277327},{"sourceId":357829,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":298136,"modelId":318745}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, Bidirectional, Attention, AdditiveAttention, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.saving import register_keras_serializable\nfrom tensorflow.keras.mixed_precision import set_global_policy\nimport json\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Verificar se há GPU disponível\nif tf.config.list_physical_devices('GPU'):\n    print(\"✅ Rodando na GPU\") \nelse:\n    print(\"⚠️ Rodando na CPU\")\n\n\n# Carregar e preparar o dataset\ndf = pd.read_csv(\"/kaggle/input/database-v2/words_and_frases_database_v8.csv\")\ndf = df.dropna()\n\n\n# Pré-processamento de Dados\n\n# Tokens especiais para o decoder\nspecial_tokens = [\"<start>\", \"<end>\"]\n\n# Criar vocabulário a partir das colunas \"english\" e \"phonetic\" e adicionar tokens especiais\nbase_vocab = set(\"\".join(df[\"english\"]) + \"\".join(df[\"phonetic\"]))\nchar_vocab = sorted(list(base_vocab)) + special_tokens\n\n# Mapeamento de caracteres para índices (iniciando em 1; 0 é reservado para padding)\nchar2idx = {char: idx for idx, char in enumerate(char_vocab, start=1)}\nidx2char = {idx: char for char, idx in char2idx.items()}\n\n\n# Função para converter texto em sequência numérica (para \"english\")\ndef text_to_seq(text):\n    return [char2idx[char] for char in text]\n\n# Função para converter texto em sequência numérica com tokens especiais (para \"phonetic\")\ndef text_to_seq_phonetic(text):\n    return [char2idx[\"<start>\"]] + [char2idx[char] for char in text] + [char2idx[\"<end>\"]]\n\n# Converter as colunas\ndf[\"english_seq\"] = df[\"english\"].apply(text_to_seq)\ndf[\"phonetic_seq\"] = df[\"phonetic\"].apply(text_to_seq_phonetic)\n\n# Preparar os inputs para o decoder:\n# - decoder_input: sequência phonetic sem o token final\n# - decoder_target: sequência phonetic sem o token de início\ndf[\"decoder_input\"] = df[\"phonetic_seq\"].apply(lambda seq: seq[:-1])\ndf[\"decoder_target\"] = df[\"phonetic_seq\"].apply(lambda seq: seq[1:])\n\n# Aplicar padding nas sequências\nmax_length_encoder = df[\"english_seq\"].apply(len).max()\nmax_length_decoder = df[\"decoder_input\"].apply(len).max()\n\ndf[\"english_seq\"] = pad_sequences(df[\"english_seq\"], maxlen=max_length_encoder, padding=\"post\").tolist()\ndf[\"decoder_input\"] = pad_sequences(df[\"decoder_input\"], maxlen=max_length_decoder, padding=\"post\").tolist()\ndf[\"decoder_target\"] = pad_sequences(df[\"decoder_target\"], maxlen=max_length_decoder, padding=\"post\").tolist()\n\n\n# 1. Categorizar as frases por tamanho\ndef categorizar_tamanho(frase):\n    n = len(frase.split())\n    if n == 1:\n        return \"Única palavra\"\n    elif 2 <= n <= 3:\n        return \"Curta (2–3 palavras)\"\n    elif 4 <= n <= 6:\n        return \"Média (4–6 palavras)\"\n    else:\n        return \"Longa (7+ palavras)\"\n\ndf[\"categoria_tamanho\"] = df[\"english\"].apply(categorizar_tamanho)\n\n# 2. Divisão estratificada\ntrain_data, test_data = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df[\"categoria_tamanho\"],\n    random_state=40\n)\n\n\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\n\n# Criar tf.data.Datasets utilizando os conjuntos separados\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    ((train_data[\"english_seq\"].tolist(), train_data[\"decoder_input\"].tolist()), train_data[\"decoder_target\"].tolist())\n)\ntrain_dataset = train_dataset.shuffle(10000).padded_batch(256, padded_shapes=(([None], [None]), [None]))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    ((test_data[\"english_seq\"].tolist(), test_data[\"decoder_input\"].tolist()), test_data[\"decoder_target\"].tolist())\n)\ntest_dataset = test_dataset.padded_batch(256, padded_shapes=(([None], [None]), [None]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Definição do Modelo Seq2Seq com Atenção e Teacher Forcing\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Attention\nimport tensorflow as tf\n\nclass Seq2Seq(Model):\n    def __init__(self, vocab_size, embedding_dim, units):\n        super().__init__()\n        self.vocab_size = vocab_size   \n        self.embedding_dim = embedding_dim\n        self.units = units\n\n        self.embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)\n        self.encoder = Bidirectional(LSTM(units, return_sequences=True, return_state=True, dropout=0.4))\n        self.decoder = LSTM(units * 2, return_sequences=True, return_state=True, dropout=0.4)\n        self.attention = Attention()\n        self.fc = Dense(vocab_size, activation='softmax') \n\n    def call(self, inputs):\n        encoder_input, decoder_input = inputs\n        enc_emb = self.embedding(encoder_input)\n        dec_emb = self.embedding(decoder_input)\n\n        enc_output, forward_h, forward_c, backward_h, backward_c = self.encoder(enc_emb)\n        state_h = tf.concat([forward_h, backward_h], axis=-1)\n        state_c = tf.concat([forward_c, backward_c], axis=-1)\n\n        dec_output, _, _ = self.decoder(dec_emb, initial_state=[state_h, state_c])\n\n        # Melhorando a máscara\n        encoder_mask = tf.cast(tf.math.not_equal(encoder_input, 0), tf.float32)\n        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n\n        context_vector = self.attention([dec_output, enc_output], mask=[None, encoder_mask])\n        combined = tf.concat([dec_output, context_vector], axis=-1)\n        output = self.fc(combined)\n        return output\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"vocab_size\": self.vocab_size,\n            \"embedding_dim\": self.embedding_dim,\n            \"units\": self.units\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(\n            vocab_size=config[\"vocab_size\"],\n            embedding_dim=config[\"embedding_dim\"],\n            units=config[\"units\"]\n        )\n\n\n\n# Configuração e Treinamento\n\nembedding_dim = 256\nunits = 256\nvocab_size = len(char2idx) + 1  # +1 para o padding (índice 0)\n\n    \nmodel = Seq2Seq(vocab_size, embedding_dim, units)\noptimizer = AdamW(learning_rate=0.0005, weight_decay=1e-4)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nmodel.compile(optimizer=optimizer, loss=loss_fn)\nmodel.summary()\n\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5)\n\n\nepochs = 100\nmodel.fit(train_dataset, \n          validation_data=test_dataset,\n          callbacks=[early_stopping, lr_scheduler],\n          epochs=epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"phonetic_transcriber_20.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}